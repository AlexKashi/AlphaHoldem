{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import holdem\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from include import *\n",
    "import matplotlib.pyplot as plt\n",
    "from libs import plotting\n",
    "import sys\n",
    "import utilities\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "with_render = True\n",
    "\n",
    "\n",
    "def get_action_policy(player_infos, community_infos, community_cards, env, _round, n_seats, state, policy):\n",
    "\tplayer_actions = None\n",
    "\tcurrent_player = community_infos[-1]\n",
    "\tplayer_object = env._player_dict[current_player]\n",
    "\tto_call = community_infos[-2]\n",
    "\tempty, seat, stack, is_playing_hand, hand_rank, played_this_round, betting, allin, lastsidepot = player_infos[current_player]\n",
    "\tplayer_object.he.set_community_cards(community_cards, _round)\n",
    "\t\n",
    "\tif _round is not \"Preflop\": # preflop already evaluated\n",
    "\t\tplayer_object.he.evaluate(_round)\n",
    "\trange_structure = utilities.fill_range_structure(_round, player_object)\n",
    "\tutilities.assign_evals_player(player_object, _round, env)\n",
    "\n",
    "\tif(current_player == 0): # learner move \n",
    "\t\tprobs = policy(state)\n",
    "\t\tchoice = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\t\tbest_nonlearning_action = player_object.choose_action(_round, range_structure, env) # Doesn't use\n",
    "\t\tif choice is 1:\n",
    "\t\t\ttotal_bet = env._tocall + env._bigblind - env.opponent.currentbet\n",
    "\t\t\tchoice = (2, total_bet)\n",
    "\t\tplayer_actions = holdem.safe_actions(community_infos, which_action=None, n_seats=n_seats, choice=choice)\n",
    "\t\t\n",
    "\telse: # bot move \n",
    "\t\t\n",
    "\t\twhich_action = player_object.choose_action(_round, range_structure, env) \n",
    "\t\tplayer_actions = holdem.safe_actions(community_infos, which_action, n_seats=n_seats, choice=None)\n",
    "\t\n",
    "\treturn player_actions\n",
    "\n",
    "\n",
    "\n",
    "def generate_episode(env, n_seats):\n",
    "\t# state observation\n",
    "\tepisode = []\n",
    "\t(player_states, (community_infos, community_cards)) = env.reset()\n",
    "\t(player_infos, player_hands) = zip(*player_states)\n",
    "\tcurrent_state = ((player_infos, player_hands), (community_infos, community_cards))\n",
    "\n",
    "\tenv.render(mode='human', initial=True)\n",
    "\tterminal = False\n",
    "\twhile not terminal:\n",
    "\n",
    "\t\t_round = utilities.which_round(community_cards)\n",
    "\t\tcurrent_player = community_infos[-1]\n",
    "\t\ta = (env._current_player.currentbet)\n",
    "\t\tactions = get_action_policy(player_infos, community_infos, community_cards, env, _round, n_seats)\n",
    "\t\t(player_states, (community_infos, community_cards)), action, rewards, terminal, info = env.step(actions)\n",
    "\t\tcurrent_state = (player_states, (community_infos, community_cards))\n",
    "\t\tepisode.append((current_state, action, env.learner_bot.reward))\n",
    "\t\tenv.render(mode='human')\n",
    "\n",
    "\treturn episode\n",
    "\n",
    "def simulate_episodes_with_graphs(no_of_episodes=100):\n",
    "\tepisode_list = []\n",
    "\tstacks_over_time = {}\n",
    "\tfor index, player in env._player_dict.items():\n",
    "\t\tstacks_over_time.update({player.get_seat(): [player.stack]})\n",
    "\tfor i in range(no_of_episodes):\n",
    "\t\tprint(\"\\n\\n********{}*********\".format(i))\n",
    "\t\tepisode = generate_episode(env, env.n_seats) \n",
    "\t\tutilities.do_necessary_env_cleanup(env)\n",
    "\t\tstack_list = env.report_game(requested_attributes = [\"stack\"])\n",
    "\t\tcount_existing_players = 0\n",
    "\n",
    "\t\tfor stack_record_index, stack_record in env._player_dict.items():\n",
    "\t\t\tarr = stacks_over_time[stack_record_index] + [stack_list[stack_record_index]]\n",
    "\t\t\tstacks_over_time.update({stack_record_index: arr})\n",
    "\t\t\tif(stack_list[stack_record_index] != 0):\n",
    "\t\t\t\tcount_existing_players += 1\n",
    "\t\tepisode_list.append(episode)\n",
    "\n",
    "\t\tif(count_existing_players == 1):\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\n",
    "\tfor player_idx, stack in stacks_over_time.items():\n",
    "\t\tif player_idx == 0:\n",
    "\t\t\tplt.plot(stack, label = \"Player {} - Learner\".format(player_idx))\n",
    "\t\telse:\t\n",
    "\t\t\tplt.plot(stack, label = \"Player {}\".format(player_idx))\n",
    "\n",
    "\tplt.ylabel('Stack Size')\n",
    "\tplt.xlabel('Episode')\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "def mc_prediction_poker(total_episodes):\n",
    "   \n",
    "    returns_sum = defaultdict(float)\n",
    "    states_count = defaultdict(float)\n",
    "    \n",
    "    V = defaultdict(float)\n",
    "    for k in range(1, total_episodes + 1):\n",
    "        print(\"\\n\\n********{}*********\".format(k))\n",
    "        episode = generate_episode(env, env.n_seats)\n",
    "        utilities.do_necessary_env_cleanup(env)\n",
    "        possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist()) # Here we create an hot encoded version of our actions\n",
    "\n",
    "        # (PSEUDOCODE)\n",
    "        # MODEL HYPERPARAMETERS: \n",
    "        # state_size = [(position, learner.stack, learner.handrank, played_this_round ...[card1, card2]), (pot_total, learner.to_call, opponent.stack, community_cards)]\n",
    "        # action_size = env.action_space.n\n",
    "        # learning_rate = 0.00025\n",
    "\n",
    "        \n",
    "\t\t\n",
    "        player_features_tuples = []\n",
    "        player_cards_tuples = []\n",
    "        community_state_tuples = []\n",
    "        for idx, sar in enumerate(episode):\n",
    "            pf = sar[0][0][0][0]\n",
    "            player_features = tuple(pf)\n",
    "            player_features_tuples.append(player_features)\n",
    "\n",
    "            pf = sar[0][0][0][1]\n",
    "            player_cards = tuple(pf)\n",
    "            player_cards_tuples.append(player_cards)\n",
    "\n",
    "            pf = sar[0][1][0]\n",
    "            community_state = tuple(pf)\n",
    "            community_state_tuples.append(community_state)\n",
    "\n",
    "        # states_in_episode = list(set([sar[0] for sar in episode])) # sar--> state,action,reward\n",
    "        states = []\n",
    "        for i in range(len(player_features_tuples)):\n",
    "            my_tup = (player_features_tuples[i] + player_cards_tuples[i] + community_state_tuples[i])\n",
    "            states.append(my_tup)\n",
    "\n",
    "        states_in_episode = set([state for state in states])\n",
    "\n",
    "        for i,state in enumerate(states_in_episode):\n",
    "            \n",
    "            G = sum([sar[2] for i,sar in enumerate(episode[i:])])\n",
    "            \n",
    "            # for stationary problems \n",
    "            returns_sum[state] += G\n",
    "            states_count[state] += 1.0         \n",
    "            V[state] = returns_sum[state] / states_count[state]\n",
    "            # end updating V\n",
    "            \n",
    "            #                    OR\n",
    "            # V[state] = V[state]+ 1/states_count[state]*(G-V[state])\n",
    "            \n",
    "            # for non stationary problems \n",
    "            #alpha=0.5\n",
    "            #V[state] = V[state]+ alpha*(G-V[state])\n",
    "            \n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "env = gym.make('TexasHoldem-v1') # holdem.TexasHoldemEnv(2)\n",
    "env.add_player(0, stack=2000) # add a player to seat 0 with 2000 \"chips\"\n",
    "# env.add_player(1, stack=2000) # tight\n",
    "env.add_player(2, stack=2000) # aggressive\n",
    "\n",
    "\n",
    "\n",
    "# v = mc_prediction_poker(10)\n",
    "# # for line_no, line in enumerate(v.items()):\n",
    "# #     print(line_no, line)\n",
    "\n",
    "# plotting.plot_value_function(v, title=\"10 Steps\")\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation): # [call/check, raise/bet, fold]\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        b = Q[observation]\n",
    "        best_action = np.argmax(b)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn\n",
    "\n",
    "def mc_control_epsilon_greedy(num_episodes, discount_factor=1.0, epsilon=0.1, is_with_rendering=with_render):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 10 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        # episode = generate_episode_control(env, env.n_seats, policy)\n",
    "\n",
    "        episode = []\n",
    "        (player_states, (community_infos, community_cards)) = env.reset()\n",
    "        (player_infos, player_hands) = zip(*player_states)\n",
    "        current_state = ((player_infos, player_hands), (community_infos, community_cards))\n",
    "\n",
    "        # Only want the state set that is relevant to learner bot every step. \n",
    "        state_set = utilities.convert_list_to_tupleA(player_states[env.learner_bot.get_seat()], current_state[1])\n",
    "\n",
    "        if is_with_rendering:\n",
    "            env.render(mode='human', initial=True)\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "\n",
    "            _round = utilities.which_round(community_cards)\n",
    "            current_player = community_infos[-1]\n",
    "            a = (env._current_player.currentbet)\n",
    "            action = get_action_policy(player_infos, community_infos, community_cards, env, _round, env.n_seats, state_set, policy)\n",
    "            \n",
    "            (player_states, (community_infos, community_cards)), action, rewards, terminal, info = env.step(action)\n",
    "\n",
    "            parsed_return_state = utilities.convert_step_return_to_set((current_state, action, env.learner_bot.reward))\n",
    "            action = utilities.convert_step_return_to_action(action)\n",
    "            episode.append((parsed_return_state, action, env.learner_bot.reward))\n",
    "            current_state = (player_states, (community_infos, community_cards)) # state = next_state\n",
    "            if is_with_rendering:\n",
    "                env.render(mode='human')\n",
    "\n",
    "        utilities.do_necessary_env_cleanup(env) # assign new positions, remove players if stack < 0 etc ..\n",
    "\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(tuple(sar[0]), sar[1]) for sar in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            state = state[0]\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0][0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return Q, policy\n",
    "\n",
    "Q, policy = mc_control_epsilon_greedy(num_episodes=100, epsilon=0.1)\n",
    "\n",
    "\n",
    "for item in Q.items():\n",
    "    print(item)\n",
    "\n",
    "# Here we have a Q-table defined which allows us to reference state-action pairs from our poker environment,\n",
    "# each state-action pair informing the agent on which action led to achieving the optimal policy. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
